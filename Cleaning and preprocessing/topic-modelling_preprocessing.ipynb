{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "216b5a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d54dbb0",
   "metadata": {},
   "source": [
    "## Preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "528ff9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ds_sentiment/reddit_climatechange_dataset_sentiment.csv', low_memory=False, lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "824c6b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116de1e0",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8caed525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all text to lowercase.\n",
    "df_pp['body-tm'] = df_pp['body'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f430251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove some more characters. This should be done in the general preprocessing before!!\n",
    "df_pp['body-tm'] = df_pp['body-tm'].str.replace('\\n', '')\n",
    "df_pp['body-tm'] = df_pp['body-tm'].str.replace('&#x200b;', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25c9c573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1950379\n"
     ]
    }
   ],
   "source": [
    "#removing nulls?\n",
    "mask = df_pp['body-tm'].notnull()\n",
    "df_pp = df_pp[mask]\n",
    "print(len(df_pp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4307eafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/yrt1h_5d5974tvcf43r84dvr0000gn/T/ipykernel_54067/4196850766.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pp['body-tm'] = df_pp['body-tm'].apply(tokenize_text)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>utc_datetime_str</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>vader_sentiment_compound</th>\n",
       "      <th>sentiment_vader</th>\n",
       "      <th>body-tm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1359582044</td>\n",
       "      <td>2013-01-30 21:40:44</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c86hmqz</td>\n",
       "      <td>The climate change denier analogy that I made ...</td>\n",
       "      <td>0.6948</td>\n",
       "      <td>positive</td>\n",
       "      <td>[the, climate, change, denier, analogy, that, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1359576987</td>\n",
       "      <td>2013-01-30 20:16:27</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c86fpmd</td>\n",
       "      <td>It seems to be getting more attention than tho...</td>\n",
       "      <td>-0.7982</td>\n",
       "      <td>negative</td>\n",
       "      <td>[it, seems, to, be, getting, more, attention, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1359574847</td>\n",
       "      <td>2013-01-30 19:40:47</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c86ewz9</td>\n",
       "      <td>Build a new one in Antarctica. Underground, or...</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>positive</td>\n",
       "      <td>[build, a, new, one, in, antarctica, ., underg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1359566007</td>\n",
       "      <td>2013-01-30 17:13:27</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c86bu1y</td>\n",
       "      <td>Fucking climate change. It disgusts me that so...</td>\n",
       "      <td>-0.0644</td>\n",
       "      <td>negative</td>\n",
       "      <td>[fucking, climate, change, ., it, disgusts, me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1359565884</td>\n",
       "      <td>2013-01-30 17:11:24</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c86bsnj</td>\n",
       "      <td>I stress about climate change, and worry about...</td>\n",
       "      <td>-0.6908</td>\n",
       "      <td>negative</td>\n",
       "      <td>[i, stress, about, climate, change, ,, and, wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  created_utc     utc_datetime_str  subreddit       id   \n",
       "0  1359582044  2013-01-30 21:40:44  AskReddit  c86hmqz  \\\n",
       "1  1359576987  2013-01-30 20:16:27  AskReddit  c86fpmd   \n",
       "2  1359574847  2013-01-30 19:40:47  AskReddit  c86ewz9   \n",
       "3  1359566007  2013-01-30 17:13:27  AskReddit  c86bu1y   \n",
       "4  1359565884  2013-01-30 17:11:24  AskReddit  c86bsnj   \n",
       "\n",
       "                                                body   \n",
       "0  The climate change denier analogy that I made ...  \\\n",
       "1  It seems to be getting more attention than tho...   \n",
       "2  Build a new one in Antarctica. Underground, or...   \n",
       "3  Fucking climate change. It disgusts me that so...   \n",
       "4  I stress about climate change, and worry about...   \n",
       "\n",
       "   vader_sentiment_compound sentiment_vader   \n",
       "0                    0.6948        positive  \\\n",
       "1                   -0.7982        negative   \n",
       "2                    0.2263        positive   \n",
       "3                   -0.0644        negative   \n",
       "4                   -0.6908        negative   \n",
       "\n",
       "                                             body-tm  \n",
       "0  [the, climate, change, denier, analogy, that, ...  \n",
       "1  [it, seems, to, be, getting, more, attention, ...  \n",
       "2  [build, a, new, one, in, antarctica, ., underg...  \n",
       "3  [fucking, climate, change, ., it, disgusts, me...  \n",
       "4  [i, stress, about, climate, change, ,, and, wo...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text using the NLTK library.\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "df_pp['body-tm'] = df_pp['body-tm'].apply(tokenize_text)\n",
    "\n",
    "df_pp.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98012f19",
   "metadata": {},
   "source": [
    "### Stop-word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31f7dfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_swr = df_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cefe1cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/yrt1h_5d5974tvcf43r84dvr0000gn/T/ipykernel_54067/2953859983.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_swr['body-tm'] = df_swr['body-tm'].apply(lambda x: [word for word in x if word not in stopwords])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>utc_datetime_str</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>vader_sentiment_compound</th>\n",
       "      <th>sentiment_vader</th>\n",
       "      <th>body-tm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1359582044</td>\n",
       "      <td>2013-01-30 21:40:44</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c86hmqz</td>\n",
       "      <td>The climate change denier analogy that I made ...</td>\n",
       "      <td>0.6948</td>\n",
       "      <td>positive</td>\n",
       "      <td>[climate, change, denier, analogy, made, perfe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1359576987</td>\n",
       "      <td>2013-01-30 20:16:27</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c86fpmd</td>\n",
       "      <td>It seems to be getting more attention than tho...</td>\n",
       "      <td>-0.7982</td>\n",
       "      <td>negative</td>\n",
       "      <td>[seems, getting, attention, issues, lately, ,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1359574847</td>\n",
       "      <td>2013-01-30 19:40:47</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c86ewz9</td>\n",
       "      <td>Build a new one in Antarctica. Underground, or...</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>positive</td>\n",
       "      <td>[build, new, one, antarctica, ., underground, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1359566007</td>\n",
       "      <td>2013-01-30 17:13:27</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c86bu1y</td>\n",
       "      <td>Fucking climate change. It disgusts me that so...</td>\n",
       "      <td>-0.0644</td>\n",
       "      <td>negative</td>\n",
       "      <td>[fucking, climate, change, ., disgusts, many, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1359565884</td>\n",
       "      <td>2013-01-30 17:11:24</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c86bsnj</td>\n",
       "      <td>I stress about climate change, and worry about...</td>\n",
       "      <td>-0.6908</td>\n",
       "      <td>negative</td>\n",
       "      <td>[stress, climate, change, ,, worry, future, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  created_utc     utc_datetime_str  subreddit       id   \n",
       "0  1359582044  2013-01-30 21:40:44  AskReddit  c86hmqz  \\\n",
       "1  1359576987  2013-01-30 20:16:27  AskReddit  c86fpmd   \n",
       "2  1359574847  2013-01-30 19:40:47  AskReddit  c86ewz9   \n",
       "3  1359566007  2013-01-30 17:13:27  AskReddit  c86bu1y   \n",
       "4  1359565884  2013-01-30 17:11:24  AskReddit  c86bsnj   \n",
       "\n",
       "                                                body   \n",
       "0  The climate change denier analogy that I made ...  \\\n",
       "1  It seems to be getting more attention than tho...   \n",
       "2  Build a new one in Antarctica. Underground, or...   \n",
       "3  Fucking climate change. It disgusts me that so...   \n",
       "4  I stress about climate change, and worry about...   \n",
       "\n",
       "   vader_sentiment_compound sentiment_vader   \n",
       "0                    0.6948        positive  \\\n",
       "1                   -0.7982        negative   \n",
       "2                    0.2263        positive   \n",
       "3                   -0.0644        negative   \n",
       "4                   -0.6908        negative   \n",
       "\n",
       "                                             body-tm  \n",
       "0  [climate, change, denier, analogy, made, perfe...  \n",
       "1  [seems, getting, attention, issues, lately, ,,...  \n",
       "2  [build, new, one, antarctica, ., underground, ...  \n",
       "3  [fucking, climate, change, ., disgusts, many, ...  \n",
       "4     [stress, climate, change, ,, worry, future, .]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stopwords using the NLTK library.\n",
    "stopwords = stopwords.words('english')\n",
    "df_swr['body-tm'] = df_swr['body-tm'].apply(lambda x: [word for word in x if word not in stopwords])\n",
    "\n",
    "df_swr.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2518b2",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23b2ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stem = df_swr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7582bff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/yrt1h_5d5974tvcf43r84dvr0000gn/T/ipykernel_54067/1756521796.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stem['body-tm'] = df_stem['body-tm'].apply(lambda x: [lemmatizer.lemmatize(token) for token in x])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>utc_datetime_str</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>vader_sentiment_compound</th>\n",
       "      <th>sentiment_vader</th>\n",
       "      <th>body-tm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1359582044</td>\n",
       "      <td>2013-01-30 21:40:44</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c86hmqz</td>\n",
       "      <td>The climate change denier analogy that I made ...</td>\n",
       "      <td>0.6948</td>\n",
       "      <td>positive</td>\n",
       "      <td>[climate, change, denier, analogy, made, perfe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1359576987</td>\n",
       "      <td>2013-01-30 20:16:27</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c86fpmd</td>\n",
       "      <td>It seems to be getting more attention than tho...</td>\n",
       "      <td>-0.7982</td>\n",
       "      <td>negative</td>\n",
       "      <td>[seems, getting, attention, issue, lately, ,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1359574847</td>\n",
       "      <td>2013-01-30 19:40:47</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c86ewz9</td>\n",
       "      <td>Build a new one in Antarctica. Underground, or...</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>positive</td>\n",
       "      <td>[build, new, one, antarctica, ., underground, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1359566007</td>\n",
       "      <td>2013-01-30 17:13:27</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c86bu1y</td>\n",
       "      <td>Fucking climate change. It disgusts me that so...</td>\n",
       "      <td>-0.0644</td>\n",
       "      <td>negative</td>\n",
       "      <td>[fucking, climate, change, ., disgust, many, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1359565884</td>\n",
       "      <td>2013-01-30 17:11:24</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c86bsnj</td>\n",
       "      <td>I stress about climate change, and worry about...</td>\n",
       "      <td>-0.6908</td>\n",
       "      <td>negative</td>\n",
       "      <td>[stress, climate, change, ,, worry, future, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  created_utc     utc_datetime_str  subreddit       id   \n",
       "0  1359582044  2013-01-30 21:40:44  AskReddit  c86hmqz  \\\n",
       "1  1359576987  2013-01-30 20:16:27  AskReddit  c86fpmd   \n",
       "2  1359574847  2013-01-30 19:40:47  AskReddit  c86ewz9   \n",
       "3  1359566007  2013-01-30 17:13:27  AskReddit  c86bu1y   \n",
       "4  1359565884  2013-01-30 17:11:24  AskReddit  c86bsnj   \n",
       "\n",
       "                                                body   \n",
       "0  The climate change denier analogy that I made ...  \\\n",
       "1  It seems to be getting more attention than tho...   \n",
       "2  Build a new one in Antarctica. Underground, or...   \n",
       "3  Fucking climate change. It disgusts me that so...   \n",
       "4  I stress about climate change, and worry about...   \n",
       "\n",
       "   vader_sentiment_compound sentiment_vader   \n",
       "0                    0.6948        positive  \\\n",
       "1                   -0.7982        negative   \n",
       "2                    0.2263        positive   \n",
       "3                   -0.0644        negative   \n",
       "4                   -0.6908        negative   \n",
       "\n",
       "                                             body-tm  \n",
       "0  [climate, change, denier, analogy, made, perfe...  \n",
       "1  [seems, getting, attention, issue, lately, ,, ...  \n",
       "2  [build, new, one, antarctica, ., underground, ...  \n",
       "3  [fucking, climate, change, ., disgust, many, p...  \n",
       "4     [stress, climate, change, ,, worry, future, .]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem the words using the NLTK library.\n",
    "# stemmer = PorterStemmer()\n",
    "# df_stem['body-tm'] = df_stem['body-tm'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "# df_stem.head(5)\n",
    "\n",
    "# trying lemmatizer instead\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df_stem['body-tm'] = df_stem['body-tm'].apply(lambda x: [lemmatizer.lemmatize(token) for token in x])\n",
    "df_stem.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcb28e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/yrt1h_5d5974tvcf43r84dvr0000gn/T/ipykernel_54067/989642176.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stem['body-tm'] = df_stem['body-tm'].apply(lambda x: ' '.join(x))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>utc_datetime_str</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>vader_sentiment_compound</th>\n",
       "      <th>sentiment_vader</th>\n",
       "      <th>body-tm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1359582044</td>\n",
       "      <td>2013-01-30 21:40:44</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c86hmqz</td>\n",
       "      <td>The climate change denier analogy that I made ...</td>\n",
       "      <td>0.6948</td>\n",
       "      <td>positive</td>\n",
       "      <td>climate change denier analogy made perfect . g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1359576987</td>\n",
       "      <td>2013-01-30 20:16:27</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c86fpmd</td>\n",
       "      <td>It seems to be getting more attention than tho...</td>\n",
       "      <td>-0.7982</td>\n",
       "      <td>negative</td>\n",
       "      <td>seems getting attention issue lately , feel li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1359574847</td>\n",
       "      <td>2013-01-30 19:40:47</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c86ewz9</td>\n",
       "      <td>Build a new one in Antarctica. Underground, or...</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>positive</td>\n",
       "      <td>build new one antarctica . underground , wait ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1359566007</td>\n",
       "      <td>2013-01-30 17:13:27</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c86bu1y</td>\n",
       "      <td>Fucking climate change. It disgusts me that so...</td>\n",
       "      <td>-0.0644</td>\n",
       "      <td>negative</td>\n",
       "      <td>fucking climate change . disgust many people n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1359565884</td>\n",
       "      <td>2013-01-30 17:11:24</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c86bsnj</td>\n",
       "      <td>I stress about climate change, and worry about...</td>\n",
       "      <td>-0.6908</td>\n",
       "      <td>negative</td>\n",
       "      <td>stress climate change , worry future .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  created_utc     utc_datetime_str  subreddit       id   \n",
       "0  1359582044  2013-01-30 21:40:44  AskReddit  c86hmqz  \\\n",
       "1  1359576987  2013-01-30 20:16:27  AskReddit  c86fpmd   \n",
       "2  1359574847  2013-01-30 19:40:47  AskReddit  c86ewz9   \n",
       "3  1359566007  2013-01-30 17:13:27  AskReddit  c86bu1y   \n",
       "4  1359565884  2013-01-30 17:11:24  AskReddit  c86bsnj   \n",
       "\n",
       "                                                body   \n",
       "0  The climate change denier analogy that I made ...  \\\n",
       "1  It seems to be getting more attention than tho...   \n",
       "2  Build a new one in Antarctica. Underground, or...   \n",
       "3  Fucking climate change. It disgusts me that so...   \n",
       "4  I stress about climate change, and worry about...   \n",
       "\n",
       "   vader_sentiment_compound sentiment_vader   \n",
       "0                    0.6948        positive  \\\n",
       "1                   -0.7982        negative   \n",
       "2                    0.2263        positive   \n",
       "3                   -0.0644        negative   \n",
       "4                   -0.6908        negative   \n",
       "\n",
       "                                             body-tm  \n",
       "0  climate change denier analogy made perfect . g...  \n",
       "1  seems getting attention issue lately , feel li...  \n",
       "2  build new one antarctica . underground , wait ...  \n",
       "3  fucking climate change . disgust many people n...  \n",
       "4             stress climate change , worry future .  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Join in a string\n",
    "df_stem['body-tm'] = df_stem['body-tm'].apply(lambda x: ' '.join(x))\n",
    "df_stem.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35160caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/yrt1h_5d5974tvcf43r84dvr0000gn/T/ipykernel_54067/1995692049.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stem['body-tm'] = df_stem['body-tm'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>utc_datetime_str</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>vader_sentiment_compound</th>\n",
       "      <th>sentiment_vader</th>\n",
       "      <th>body-tm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1359582044</td>\n",
       "      <td>2013-01-30 21:40:44</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c86hmqz</td>\n",
       "      <td>The climate change denier analogy that I made ...</td>\n",
       "      <td>0.6948</td>\n",
       "      <td>positive</td>\n",
       "      <td>climate change denier analogy made perfect  ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1359576987</td>\n",
       "      <td>2013-01-30 20:16:27</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c86fpmd</td>\n",
       "      <td>It seems to be getting more attention than tho...</td>\n",
       "      <td>-0.7982</td>\n",
       "      <td>negative</td>\n",
       "      <td>seems getting attention issue lately  feel lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1359574847</td>\n",
       "      <td>2013-01-30 19:40:47</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c86ewz9</td>\n",
       "      <td>Build a new one in Antarctica. Underground, or...</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>positive</td>\n",
       "      <td>build new one antarctica  underground  wait ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1359566007</td>\n",
       "      <td>2013-01-30 17:13:27</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c86bu1y</td>\n",
       "      <td>Fucking climate change. It disgusts me that so...</td>\n",
       "      <td>-0.0644</td>\n",
       "      <td>negative</td>\n",
       "      <td>fucking climate change  disgust many people nt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1359565884</td>\n",
       "      <td>2013-01-30 17:11:24</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c86bsnj</td>\n",
       "      <td>I stress about climate change, and worry about...</td>\n",
       "      <td>-0.6908</td>\n",
       "      <td>negative</td>\n",
       "      <td>stress climate change  worry future</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  created_utc     utc_datetime_str  subreddit       id   \n",
       "0  1359582044  2013-01-30 21:40:44  AskReddit  c86hmqz  \\\n",
       "1  1359576987  2013-01-30 20:16:27  AskReddit  c86fpmd   \n",
       "2  1359574847  2013-01-30 19:40:47  AskReddit  c86ewz9   \n",
       "3  1359566007  2013-01-30 17:13:27  AskReddit  c86bu1y   \n",
       "4  1359565884  2013-01-30 17:11:24  AskReddit  c86bsnj   \n",
       "\n",
       "                                                body   \n",
       "0  The climate change denier analogy that I made ...  \\\n",
       "1  It seems to be getting more attention than tho...   \n",
       "2  Build a new one in Antarctica. Underground, or...   \n",
       "3  Fucking climate change. It disgusts me that so...   \n",
       "4  I stress about climate change, and worry about...   \n",
       "\n",
       "   vader_sentiment_compound sentiment_vader   \n",
       "0                    0.6948        positive  \\\n",
       "1                   -0.7982        negative   \n",
       "2                    0.2263        positive   \n",
       "3                   -0.0644        negative   \n",
       "4                   -0.6908        negative   \n",
       "\n",
       "                                             body-tm  \n",
       "0  climate change denier analogy made perfect  ge...  \n",
       "1  seems getting attention issue lately  feel lik...  \n",
       "2  build new one antarctica  underground  wait ti...  \n",
       "3  fucking climate change  disgust many people nt...  \n",
       "4               stress climate change  worry future   "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Remove punctuation using the string library.\n",
    "df_stem['body-tm'] = df_stem['body-tm'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "df_stem.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82928546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed DataFrame to a new CSV file.\n",
    "# df_stem.to_csv('ds_test/preprocessed_topicmoddeling_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3087c8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeafbcc",
   "metadata": {},
   "source": [
    "#### Removing rows with less than 2 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f025bce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/yrt1h_5d5974tvcf43r84dvr0000gn/T/ipykernel_54067/1361174534.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_count['num_words'] = df_count['body-tm'].str.split().str.len()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comments with more than 2 words: 1909336\n",
      "comments with less than 2 words: 41043\n"
     ]
    }
   ],
   "source": [
    "df_count = df_stem\n",
    "\n",
    "# Count the number of words in each datapoint\n",
    "df_count['num_words'] = df_count['body-tm'].str.split().str.len()\n",
    "df_filtered = df_count[df_count['num_words'] >= 2]\n",
    "df_filtered = df_filtered.drop(columns=['num_words'])\n",
    "\n",
    "# save the small posts to check\n",
    "df_less = df_count[df_count['num_words'] < 2]\n",
    "# df_less.to_csv('ds_test/topicmodelling_less-than-5-words-comments.csv', index=False)\n",
    "\n",
    "df_result = df_filtered\n",
    "print('comments with more than 2 words:', len(df_result))\n",
    "print('comments with less than 2 words:', len(df_less))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a528b6",
   "metadata": {},
   "source": [
    "#### Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7923e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete duplicates (should be done in the good dataset though)\n",
    "duplicate_ids = df_result[df_result.duplicated(subset=['id'], keep=False)]\n",
    "duplicates_df = pd.DataFrame(duplicate_ids)\n",
    "\n",
    "df_result.drop_duplicates(subset=['id'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62f2945",
   "metadata": {},
   "source": [
    "#### Remove topical words \n",
    "Because of bloating the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3173a856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words climate and change\n",
    "df_result['body-tm'] = df_result['body-tm'].str.replace('climate', '')\n",
    "df_result['body-tm'] = df_result['body-tm'].str.replace('change', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af819748",
   "metadata": {},
   "source": [
    "#### Remove non-topical words\n",
    "because of bloating the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da45de0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>utc_datetime_str</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>vader_sentiment_compound</th>\n",
       "      <th>sentiment_vader</th>\n",
       "      <th>body-tm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1359559914</td>\n",
       "      <td>2013-01-30 15:31:54</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>c869yg4</td>\n",
       "      <td>They can efficiently take down any civilian re...</td>\n",
       "      <td>-0.8996</td>\n",
       "      <td>negative</td>\n",
       "      <td>efficiently take civilian resistance whether b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  created_utc     utc_datetime_str  subreddit       id   \n",
       "5  1359559914  2013-01-30 15:31:54  AskReddit  c869yg4  \\\n",
       "\n",
       "                                                body   \n",
       "5  They can efficiently take down any civilian re...  \\\n",
       "\n",
       "   vader_sentiment_compound sentiment_vader   \n",
       "5                   -0.8996        negative  \\\n",
       "\n",
       "                                             body-tm  \n",
       "5  efficiently take civilian resistance whether b...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FIND COMMENTS THAT INCLUDE THE NON-TOPICAL WORDS.\n",
    "# Filter comments that include the word \"climate\"\n",
    "filtered_comments = df_result[df_result['body-tm'].str.contains(' u ', case=False)]\n",
    "filtered_comments.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10cab865",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_comments.head(5).to_csv('ds_test/analyse-non-topical-words.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b11a12cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove apostroph\n",
    "df_result['body-tm'] = df_result['body-tm'].str.replace(\"’\", \"\")\n",
    "df_result['body-tm'] = df_result['body-tm'].str.replace('”', '')\n",
    "df_result['body-tm'] = df_result['body-tm'].str.replace(\"“\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ba7bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stand alone non-topical words\n",
    "df_result['body-tm'] = df_result['body-tm'].str.replace(\" nt \", \"\")\n",
    "df_result['body-tm'] = df_result['body-tm'].str.replace(\" re \", \"\")\n",
    "df_result['body-tm'] = df_result['body-tm'].str.replace(\" m \", \"\")\n",
    "df_result['body-tm'] = df_result['body-tm'].str.replace(\" u \", \"\")\n",
    "df_result['body-tm'] = df_result['body-tm'].str.replace(\" ve \", \"\")\n",
    "df_result['body-tm'] = df_result['body-tm'].str.replace(' s ', ' ')\n",
    "df_result['body-tm'] = df_result['body-tm'].str.replace(' d ', ' ')\n",
    "df_result['body-tm'] = df_result['body-tm'].str.replace(' le ', ' ')\n",
    "df_result['body-tm'] = df_result['body-tm'].str.replace(' op ', ' ')\n",
    "df_result['body-tm'] = df_result['body-tm'].str.replace(' v ', ' ')\n",
    "\n",
    "#questionable words?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a6cc5f",
   "metadata": {},
   "source": [
    "#### Remove null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af89122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing nulls?\n",
    "mask = df_result['body-tm'].notnull()\n",
    "df_result = df_result[mask]\n",
    "print(len(df_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfc3e03",
   "metadata": {},
   "source": [
    "#### Save file to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5207b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv('ds_test/preprocessed_topicmoddeling_masterset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a093010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "# Load audio file\n",
    "audio_file = \"audio/done.mp3\"\n",
    "\n",
    "# Play audio file\n",
    "Audio(filename=audio_file, autoplay=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c66fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
