{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce1c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from psaw import PushshiftAPI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cca79d5",
   "metadata": {},
   "source": [
    "## API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6558268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmaw import PushshiftAPI\n",
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e930ddba",
   "metadata": {},
   "source": [
    "## select subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38949ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit=\"climatechange\"\n",
    "limit=1000\n",
    "\n",
    "\n",
    "\n",
    "# adding timeframe \n",
    "before = int(dt.datetime(2022,1,2,0,0).timestamp())\n",
    "after = int(dt.datetime(2022,1,1,0,0).timestamp())\n",
    "\n",
    "comments = api.search_comments(subreddit=subreddit, limit=limit, before=before, after=after)\n",
    "print(f'Retrieved {len(comments)} comments from Pushshift {subreddit}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf4abb",
   "metadata": {},
   "source": [
    "## Start scraping\n",
    "Scraping the above selected subreddit for every day. Then saving it in a csv for per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd459ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "temp_df = pd.DataFrame() #Temporary empty dataframe\n",
    "\n",
    "year = 2022\n",
    "\n",
    "# YEARLY SCRAPE\n",
    "# If year has comments < 1000 scrape everything at once \n",
    "scrapeYearDateBefore = int(dt.datetime(year,12,31,0,0).timestamp())\n",
    "scrapeYearDateAfter = int(dt.datetime(year,1,1,0,0).timestamp())\n",
    "\n",
    "# Scrape all comments per Year\n",
    "commentsYear = api.search_comments(subreddit=subreddit, limit=limit, before=scrapeYearDateBefore, after=scrapeYearDateAfter)\n",
    "\n",
    "\n",
    "if (len(commentsYear) > 0 and len(commentsYear) < 1000): \n",
    "    print(f'Retrieved {len(commentsYear)} comments in {year} from Pushshift')\n",
    "    # convert comments into a datafrom\n",
    "    comments_df = pd.DataFrame(commentsYear)\n",
    "\n",
    "    #add all datapoints to a temporary dataframe\n",
    "    temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "    \n",
    "    \n",
    "# MONTLY SCRAPE\n",
    "else:\n",
    "\n",
    "    print(f'more thann {len(commentsYear)} comments in {year} in sr: {subreddit}. Attempting monthyly scraping...')\n",
    "    \n",
    "    for m in range(1, 13):\n",
    "        yearAfter = year\n",
    "        monthBefore = m\n",
    "        monthAfter = m\n",
    "        day = 32\n",
    "        minutes = 0\n",
    "        endMonthDate = 31\n",
    "        \n",
    "        if(m == 1) or (m == 3) or (m == 5) or (m == 7) or (m == 8) or (m == 10) or (m == 12):\n",
    "            endMonthDate = 31\n",
    "        elif(m == 2):\n",
    "            endMonthDate = 28\n",
    "        else: \n",
    "            endMonthDate = 30\n",
    "            \n",
    "        # If month has comments < 1000 scrape all monthly data at once \n",
    "        scrapeMonthDateBefore = int(dt.datetime(year,m,endMonthDate,0,0).timestamp())\n",
    "        scrapeMonthDateAfter = int(dt.datetime(year,m,1,0,0).timestamp())\n",
    "        \n",
    "        # Scrape all comments per month\n",
    "        commentsMonth = api.search_comments(subreddit=subreddit, limit=limit, before=scrapeMonthDateBefore, after=scrapeMonthDateAfter)\n",
    "        \n",
    "        if (len(commentsMonth) > 0 and len(commentsMonth) < 1000): \n",
    "            print(f'Retrieved {len(commentsMonth)} comments in month: {m} - from Pushshift')\n",
    "            # convert comments into a datafrom\n",
    "            comments_df = pd.DataFrame(commentsMonth)\n",
    "\n",
    "            #add all datapoints to a temporary dataframe\n",
    "            temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "        # DAILY SCRAPE\n",
    "        else:\n",
    "            print(f'more than {len(commentsMonth)} comments in month {m} - {year}')\n",
    "            if (m == 2): #februari \n",
    "                day = 29\n",
    "            if (m == 4) or (m == 6) or (m == 9) or (m == 11): # months with 30 days -> its days +1 for the loop\n",
    "                day = 31\n",
    "\n",
    "            for d in range(1, day):\n",
    "                # date variables\n",
    "                dayBefore = d\n",
    "                dayAfter = d-1\n",
    "\n",
    "                # change dayAfter / monthAfter on certain conditions\n",
    "                if(dayAfter == 0): # \n",
    "                    monthAfter = m-1\n",
    "                    if(m == 2):\n",
    "                        dayAfter = 31\n",
    "                    elif(m==3):\n",
    "                        dayAfter = 28\n",
    "                    elif (m == 4) or (m == 6) or (m == 9) or (m == 11) or (m == 13):\n",
    "                        dayAfter = 31\n",
    "                    else: dayAfter = 30\n",
    "                else: monthAfter = m\n",
    "\n",
    "                if(monthAfter == 0):\n",
    "                    continue\n",
    "\n",
    "                #set time frame for scraping -> Scraping for every day in the year.\n",
    "                scrapeDayDateBefore = int(dt.datetime(year,monthBefore,dayBefore,0,0).timestamp())\n",
    "                scrapeDayDateAfter = int(dt.datetime(yearAfter,monthAfter,dayAfter,0,0).timestamp())\n",
    "\n",
    "\n",
    "                # Scrape all comments per day\n",
    "                commentsDay = api.search_comments(subreddit=subreddit, limit=limit, before=scrapeDayDateBefore, after=scrapeDayDateAfter)\n",
    "                  \n",
    "                # if statement day comments < 1000\n",
    "                if (len(commentsDay) < 1000): \n",
    "                    print('datebefore', dayBefore, '-', monthBefore, '- ', year, 'date after', dayAfter, '-', monthAfter, '- ', yearAfter)\n",
    "                    print(f'Retrieved {len(commentsDay)} comments per day from Pushshift')\n",
    "                    # convert comments into a datafrom\n",
    "                    comments_df = pd.DataFrame(commentsDay)\n",
    "\n",
    "                    #add all datapoints to a temporary dataframe\n",
    "                    temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "                # HOURLY SCRAPE\n",
    "                else: \n",
    "                    print(f'more than {len(commentsDay)} comments: failed daily scrape, attempting hourly scrape...')\n",
    "\n",
    "                    for h in range(24):\n",
    "\n",
    "                        # add an hour to the datetime object\n",
    "                        hourBefore = h + 1\n",
    "                        hourAfter = h\n",
    "                        minuteAfter = 0\n",
    "\n",
    "                        if (hourBefore == 24):\n",
    "                            hourBefore = 23\n",
    "                            minutes = 59\n",
    "                        else: \n",
    "                            hourBefore = h + 1\n",
    "                            minutes = 0\n",
    "\n",
    "\n",
    "\n",
    "                        #set time frame for scraping -> Scraping for every hour in the day\n",
    "                        scrapeHourDateBefore = int(dt.datetime(year,m,d,hourBefore,minutes).timestamp())\n",
    "                        scrapeHourDateAfter = int(dt.datetime(yearAfter,m,d,hourAfter,minuteAfter).timestamp())      \n",
    "\n",
    "                        # Scrape all comments per hour\n",
    "                        commentsHour = api.search_comments(subreddit=subreddit, limit=limit, before=scrapeHourDateBefore, after=scrapeHourDateAfter)\n",
    "\n",
    "                        # if statement hour comments < 1000\n",
    "                        if (len(commentsHour) < 1000): \n",
    "                            print('datebefore', dt.datetime(year,m,d,hourBefore,minutes), 'date after', dt.datetime(yearAfter,m,d,hourAfter,minuteAfter)) \n",
    "                            print(f'Retrieved {len(commentsHour)} comments per hour from Pushshift')\n",
    "                            # convert comments into a datafrom\n",
    "                            comments_df = pd.DataFrame(commentsHour)\n",
    "\n",
    "                            #add all datapoints to a temporary dataframe\n",
    "                            temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "\n",
    "\n",
    "                        # SCRAPING PER 20 MINUTES\n",
    "                        else:\n",
    "                            print(f'more than {len(commentsHour)} per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...')\n",
    "\n",
    "                            for q in range(3):\n",
    "\n",
    "                                minuteBefore = 20\n",
    "                                minuteAfter = 0\n",
    "\n",
    "                                if (q == 0):\n",
    "                                    minuteBefore = 20\n",
    "                                    minuteAfter = 0\n",
    "\n",
    "                                if (q == 1):\n",
    "                                    minuteBefore = 40\n",
    "                                    minuteAfter = 20\n",
    "\n",
    "                                if (q == 2):\n",
    "                                    minuteBefore = 59\n",
    "                                    minuteAfter = 40\n",
    "\n",
    "\n",
    "                                scrapeMinuteDateBefore = int(dt.datetime(year,m,d,h,minuteBefore).timestamp())\n",
    "                                scrapeMinuteDateAfter = int(dt.datetime(year,m,d,h,minuteAfter).timestamp())  \n",
    "\n",
    "\n",
    "                                # Scrape all comments per 20 mins\n",
    "                                commentsMinute = api.search_comments(subreddit=subreddit, limit=limit, before=scrapeMinuteDateBefore, after=scrapeMinuteDateAfter)\n",
    "\n",
    "                                # if statement hour comments < 1000\n",
    "                                if (len(commentsMinute) < 1000): \n",
    "                                    print('dateafter', dt.datetime(year,m,d,h,minuteAfter), 'date after', dt.datetime(year,m,d,h,minuteBefore)) \n",
    "                                    print(f'Retrieved {len(commentsMinute)} comments per 20 minutes from Pushshift')\n",
    "                                    # convert comments into a datafrom\n",
    "                                    comments_df = pd.DataFrame(commentsMinute)\n",
    "\n",
    "                                    #add all datapoints to a temporary dataframe\n",
    "                                    temp_df = temp_df.append(comments_df, ignore_index=True)                                   \n",
    "\n",
    "\n",
    "\n",
    "                                else:\n",
    "                                # convert comments into a datafrom\n",
    "                                    comments_df = pd.DataFrame(commentsMinute)\n",
    "\n",
    "                                    #add all datapoints to a temporary dataframe\n",
    "                                    temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "\n",
    "                                    print('ERROR MORE THAN 1000 per 20 minutes, however, still going on. INCOMPLETE')\n",
    "                                    print(f'more than {len(commentsMinute)} comments on day {d}-{m}-{year} between {h}:{minuteAfter} and {h}:{minuteBefore}')\n",
    "        #                             raise ValueError(\"More than 1000 comments in this HOUR.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                \n",
    "\n",
    "# all datapoints in temporary dataframe to one dataframe\n",
    "df1 = pd.DataFrame(temp_df)\n",
    "                                      \n",
    "# create the filename with the variable name embedded\n",
    "filename = f'{subreddit}_{year}.csv'\n",
    "\n",
    "# save the dataframe to the file with the embedded variable name\n",
    "df1.to_csv(filename, encoding='utf-8', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad95d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4321645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "temp_df = pd.DataFrame() #Temporary empty dataframe\n",
    "\n",
    "year = 2016\n",
    "\n",
    "# YEARLY SCRAPE\n",
    "# If year has comments < 1000 scrape everything at once \n",
    "scrapeYearDateBefore = int(dt.datetime(year,12,31,0,0).timestamp())\n",
    "scrapeYearDateAfter = int(dt.datetime(year,1,1,0,0).timestamp())\n",
    "\n",
    "# Scrape all comments per Year\n",
    "commentsYear = api.search_comments(subreddit=subreddit, limit=limit, before=scrapeYearDateBefore, after=scrapeYearDateAfter)\n",
    "\n",
    "\n",
    "if (len(commentsYear) > 0 and len(commentsYear) < 1000): \n",
    "    print(f'Retrieved {len(commentsYear)} comments in {year} from Pushshift')\n",
    "    # convert comments into a datafrom\n",
    "    comments_df = pd.DataFrame(commentsYear)\n",
    "\n",
    "    #add all datapoints to a temporary dataframe\n",
    "    temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "    \n",
    "    \n",
    "# MONTLY SCRAPE\n",
    "else:\n",
    "\n",
    "    print(f'more thann {len(commentsYear)} comments in {year} in sr: {subreddit}. Attempting monthyly scraping...')\n",
    "    \n",
    "    for m in range(1, 13):\n",
    "        yearAfter = year\n",
    "        monthBefore = m\n",
    "        monthAfter = m\n",
    "        day = 32\n",
    "        minutes = 0\n",
    "        endMonthDate = 31\n",
    "        \n",
    "        if(m == 1) or (m == 3) or (m == 5) or (m == 7) or (m == 8) or (m == 10) or (m == 12):\n",
    "            endMonthDate = 31\n",
    "        elif(m == 2):\n",
    "            endMonthDate = 28\n",
    "        else: \n",
    "            endMonthDate = 30\n",
    "            \n",
    "        # If month has comments < 1000 scrape all monthly data at once \n",
    "        scrapeMonthDateBefore = int(dt.datetime(year,m,endMonthDate,0,0).timestamp())\n",
    "        scrapeMonthDateAfter = int(dt.datetime(year,m,1,0,0).timestamp())\n",
    "        \n",
    "        # Scrape all comments per month\n",
    "        commentsMonth = api.search_comments(subreddit=subreddit, limit=limit, before=scrapeMonthDateBefore, after=scrapeMonthDateAfter)\n",
    "        \n",
    "        if (len(commentsMonth) > 0 and len(commentsMonth) < 1000): \n",
    "            print(f'Retrieved {len(commentsMonth)} comments in month: {m} - from Pushshift')\n",
    "            # convert comments into a datafrom\n",
    "            comments_df = pd.DataFrame(commentsMonth)\n",
    "\n",
    "            #add all datapoints to a temporary dataframe\n",
    "            temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "        # DAILY SCRAPE\n",
    "        else:\n",
    "            print(f'more than {len(commentsMonth)} comments in month {m} - {year}')\n",
    "            if (m == 2): #februari \n",
    "                day = 29\n",
    "            if (m == 4) or (m == 6) or (m == 9) or (m == 11): # months with 30 days -> its days +1 for the loop\n",
    "                day = 31\n",
    "\n",
    "            for d in range(1, day):\n",
    "                # date variables\n",
    "                dayBefore = d\n",
    "                dayAfter = d-1\n",
    "\n",
    "                # change dayAfter / monthAfter on certain conditions\n",
    "                if(dayAfter == 0): # \n",
    "                    monthAfter = m-1\n",
    "                    if(m == 2):\n",
    "                        dayAfter = 31\n",
    "                    elif(m==3):\n",
    "                        dayAfter = 28\n",
    "                    elif (m == 4) or (m == 6) or (m == 9) or (m == 11) or (m == 13):\n",
    "                        dayAfter = 31\n",
    "                    else: dayAfter = 30\n",
    "                else: monthAfter = m\n",
    "\n",
    "                if(monthAfter == 0):\n",
    "                    continue\n",
    "\n",
    "                #set time frame for scraping -> Scraping for every day in the year.\n",
    "                scrapeDayDateBefore = int(dt.datetime(year,monthBefore,dayBefore,0,0).timestamp())\n",
    "                scrapeDayDateAfter = int(dt.datetime(yearAfter,monthAfter,dayAfter,0,0).timestamp())\n",
    "\n",
    "\n",
    "                # Scrape all comments per day\n",
    "                commentsDay = api.search_comments(subreddit=subreddit, limit=limit, before=scrapeDayDateBefore, after=scrapeDayDateAfter)\n",
    "                  \n",
    "                # if statement day comments < 1000\n",
    "                if (len(commentsDay) < 1000): \n",
    "                    print('datebefore', dayBefore, '-', monthBefore, '- ', year, 'date after', dayAfter, '-', monthAfter, '- ', yearAfter)\n",
    "                    print(f'Retrieved {len(commentsDay)} comments per day from Pushshift')\n",
    "                    # convert comments into a datafrom\n",
    "                    comments_df = pd.DataFrame(commentsDay)\n",
    "\n",
    "                    #add all datapoints to a temporary dataframe\n",
    "                    temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "      \n",
    "    \n",
    "    \n",
    "            \n",
    "    \n",
    "                # HOURLY SCRAPE\n",
    "                else: \n",
    "                    print(f'more than {len(commentsDay)} comments: failed daily scrape, attempting hourly scrape...')\n",
    "\n",
    "                    for h in range(24):\n",
    "\n",
    "                        # add an hour to the datetime object\n",
    "                        hourBefore = h + 1\n",
    "                        hourAfter = h\n",
    "                        minuteAfter = 0\n",
    "\n",
    "                        if (hourBefore == 24):\n",
    "                            hourBefore = 23\n",
    "                            minutes = 59\n",
    "                        else: \n",
    "                            hourBefore = h + 1\n",
    "                            minutes = 0\n",
    "\n",
    "\n",
    "\n",
    "                        #set time frame for scraping -> Scraping for every hour in the day\n",
    "                        scrapeHourDateBefore = int(dt.datetime(year,m,d,hourBefore,minutes).timestamp())\n",
    "                        scrapeHourDateAfter = int(dt.datetime(yearAfter,m,d,hourAfter,minuteAfter).timestamp())      \n",
    "\n",
    "                        # Scrape all comments per hour\n",
    "                        commentsHour = api.search_comments(subreddit=subreddit, limit=limit, before=scrapeHourDateBefore, after=scrapeHourDateAfter)\n",
    "\n",
    "                        # if statement hour comments < 1000\n",
    "                        if (len(commentsHour) < 1000): \n",
    "                            print('datebefore', dt.datetime(year,m,d,hourBefore,minutes), 'date after', dt.datetime(yearAfter,m,d,hourAfter,minuteAfter)) \n",
    "                            print(f'Retrieved {len(commentsHour)} comments per hour from Pushshift')\n",
    "                            # convert comments into a datafrom\n",
    "                            comments_df = pd.DataFrame(commentsHour)\n",
    "\n",
    "                            #add all datapoints to a temporary dataframe\n",
    "                            temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "\n",
    "\n",
    "                        # SCRAPING PER 20 MINUTES\n",
    "                        else:\n",
    "                            print(f'more than {len(commentsHour)} per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...')\n",
    "\n",
    "                            for q in range(3):\n",
    "\n",
    "                                minuteBefore = 20\n",
    "                                minuteAfter = 0\n",
    "\n",
    "                                if (q == 0):\n",
    "                                    minuteBefore = 20\n",
    "                                    minuteAfter = 0\n",
    "\n",
    "                                if (q == 1):\n",
    "                                    minuteBefore = 40\n",
    "                                    minuteAfter = 20\n",
    "\n",
    "                                if (q == 2):\n",
    "                                    minuteBefore = 59\n",
    "                                    minuteAfter = 40\n",
    "\n",
    "\n",
    "                                scrapeMinuteDateBefore = int(dt.datetime(year,m,d,h,minuteBefore).timestamp())\n",
    "                                scrapeMinuteDateAfter = int(dt.datetime(year,m,d,h,minuteAfter).timestamp())  \n",
    "\n",
    "\n",
    "                                # Scrape all comments per 20 mins\n",
    "                                commentsMinute = api.search_comments(subreddit=subreddit, limit=limit, before=scrapeMinuteDateBefore, after=scrapeMinuteDateAfter)\n",
    "\n",
    "                                # if statement hour comments < 1000\n",
    "                                if (len(commentsMinute) < 1000): \n",
    "                                    print('dateafter', dt.datetime(year,m,d,h,minuteAfter), 'date after', dt.datetime(year,m,d,h,minuteBefore)) \n",
    "                                    print(f'Retrieved {len(commentsMinute)} comments per 20 minutes from Pushshift')\n",
    "                                    # convert comments into a datafrom\n",
    "                                    comments_df = pd.DataFrame(commentsMinute)\n",
    "\n",
    "                                    #add all datapoints to a temporary dataframe\n",
    "                                    temp_df = temp_df.append(comments_df, ignore_index=True)                                   \n",
    "\n",
    "\n",
    "\n",
    "                                else:\n",
    "                                # convert comments into a datafrom\n",
    "                                    comments_df = pd.DataFrame(commentsMinute)\n",
    "\n",
    "                                    #add all datapoints to a temporary dataframe\n",
    "                                    temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "\n",
    "                                    print('ERROR MORE THAN 1000 per 20 minutes, however, still going on. INCOMPLETE')\n",
    "                                    print(f'more than {len(commentsMinute)} comments on day {d}-{m}-{year} between {h}:{minuteAfter} and {h}:{minuteBefore}')\n",
    "        #                             raise ValueError(\"More than 1000 comments in this HOUR.\")\n",
    "\n",
    "\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "                                \n",
    "\n",
    "# all datapoints in temporary dataframe to one dataframe\n",
    "df1 = pd.DataFrame(temp_df)\n",
    "                                      \n",
    "# create the filename with the variable name embedded\n",
    "filename = f'{subreddit}_{year}.csv'\n",
    "\n",
    "# save the dataframe to the file with the embedded variable name\n",
    "df1.to_csv(filename, encoding='utf-8', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72477d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
