{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e132627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# load the CSV file into a Pandas dataframe\n",
    "df = pd.read_csv('ds_sentiment/annotation_set_sentiment_extra.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d8b3b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows where there is no agreement on sentiment with the annotators\n",
    "# Filter the rows where the \"sentiment_annotator\" column is null\n",
    "null_sentiment = df[df['sentiment_annotator'].isnull()]\n",
    "\n",
    "# Drop the rows where \"sentiment_annotator\" is null\n",
    "df.drop(null_sentiment.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "817ede17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cec4a24",
   "metadata": {},
   "source": [
    "### Accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a635b919",
   "metadata": {},
   "source": [
    "#### textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75cc8399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.95022624434389\n"
     ]
    }
   ],
   "source": [
    "total = len(df)\n",
    "\n",
    "num_positive = len(df[(df['sentiment_annotator'] == 'positive') & (df['sentiment_textblob'] == 'positive')])\n",
    "num_neutral = len(df[(df['sentiment_annotator'] == 'neutral') & (df['sentiment_textblob'] == 'neutral')])\n",
    "num_negative = len(df[(df['sentiment_annotator'] == 'negative') & (df['sentiment_textblob'] == 'negative')])\n",
    "\n",
    "total_matching = num_positive + num_neutral + num_negative\n",
    "\n",
    "accuracy = total_matching / total * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfc7b3b",
   "metadata": {},
   "source": [
    "#### textblob v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d4fa6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.081447963800905\n"
     ]
    }
   ],
   "source": [
    "total = len(df)\n",
    "\n",
    "num_positive = len(df[(df['sentiment_annotator'] == 'positive') & (df['sentiment_textblob-v2'] == 'positive')])\n",
    "num_neutral = len(df[(df['sentiment_annotator'] == 'neutral') & (df['sentiment_textblob-v2'] == 'neutral')])\n",
    "num_negative = len(df[(df['sentiment_annotator'] == 'negative') & (df['sentiment_textblob-v2'] == 'negative')])\n",
    "\n",
    "total_matching = num_positive + num_neutral + num_negative\n",
    "\n",
    "accuracy = total_matching / total * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723b6770",
   "metadata": {},
   "source": [
    "#### vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a60436a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.26244343891403\n"
     ]
    }
   ],
   "source": [
    "total = len(df)\n",
    "\n",
    "num_positive = len(df[(df['sentiment_annotator'] == 'positive') & (df['sentiment_vader'] == 'positive')])\n",
    "num_neutral = len(df[(df['sentiment_annotator'] == 'neutral') & (df['sentiment_vader'] == 'neutral')])\n",
    "num_negative = len(df[(df['sentiment_annotator'] == 'negative') & (df['sentiment_vader'] == 'negative')])\n",
    "\n",
    "total_matching = num_positive + num_neutral + num_negative\n",
    "\n",
    "accuracy = total_matching / total * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8933e476",
   "metadata": {},
   "source": [
    "## precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba671b4a",
   "metadata": {},
   "source": [
    "#### textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09db080b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision-positive: 0.22924901185770752\n",
      "precision-neutral: 0.34285714285714286\n",
      "precision-negative: 0.8319327731092437\n",
      "final precision score: 0.4680129759413647\n"
     ]
    }
   ],
   "source": [
    "positive_tp = len(df[(df['sentiment_annotator'] == 'positive') & (df['sentiment_textblob'] == 'positive')])\n",
    "positive_fp = len(df[(df['sentiment_textblob'] == 'positive') & (df['sentiment_annotator'] != 'positive')])\n",
    "# pos_test = len(df[(df['sentiment_ano'] == 'positive')])\n",
    "# print(pos_test, positive_fp, positive_tp)\n",
    "precision_positive = positive_tp / (positive_fp + positive_tp)\n",
    "\n",
    "\n",
    "neutral_tp = len(df[(df['sentiment_annotator'] == 'neutral') & (df['sentiment_textblob'] == 'neutral')])\n",
    "neutral_fp = len(df[(df['sentiment_textblob'] == 'neutral') & (df['sentiment_annotator'] != 'neutral')])\n",
    "precision_neutral = neutral_tp / (neutral_fp + neutral_tp)\n",
    "\n",
    "negative_tp = len(df[(df['sentiment_annotator'] == 'negative') & (df['sentiment_textblob'] == 'negative')])\n",
    "negative_fp = len(df[(df['sentiment_textblob'] == 'negative') & (df['sentiment_annotator'] != 'negative')])\n",
    "precision_negative = negative_tp / (negative_fp + negative_tp)\n",
    "\n",
    "\n",
    "\n",
    "print('precision-positive:', precision_positive)\n",
    "print('precision-neutral:', precision_neutral)\n",
    "print('precision-negative:', precision_negative)\n",
    "\n",
    "total_precision = (precision_positive + precision_neutral + precision_negative) / 3\n",
    "print('final precision score:', total_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b944998",
   "metadata": {},
   "source": [
    "#### vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6879d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision-positive: 0.2568306010928962\n",
      "precision-neutral: 0.35\n",
      "precision-negative: 0.8190954773869347\n",
      "final precision score: 0.4753086928266102\n"
     ]
    }
   ],
   "source": [
    "positive_tp = len(df[(df['sentiment_annotator'] == 'positive') & (df['sentiment_vader'] == 'positive')])\n",
    "positive_fp = len(df[(df['sentiment_vader'] == 'positive') & (df['sentiment_annotator'] != 'positive')])\n",
    "precision_positive = positive_tp / (positive_fp + positive_tp)\n",
    "\n",
    "\n",
    "neutral_tp = len(df[(df['sentiment_annotator'] == 'neutral') & (df['sentiment_vader'] == 'neutral')])\n",
    "neutral_fp = len(df[(df['sentiment_vader'] == 'neutral') & (df['sentiment_annotator'] != 'neutral')])\n",
    "precision_neutral = neutral_tp / (neutral_fp + neutral_tp)\n",
    "\n",
    "negative_tp = len(df[(df['sentiment_annotator'] == 'negative') & (df['sentiment_vader'] == 'negative')])\n",
    "negative_fp = len(df[(df['sentiment_vader'] == 'negative') & (df['sentiment_annotator'] != 'negative')])\n",
    "precision_negative = negative_tp / (negative_fp + negative_tp)\n",
    "\n",
    "\n",
    "\n",
    "print('precision-positive:', precision_positive)\n",
    "print('precision-neutral:', precision_neutral)\n",
    "print('precision-negative:', precision_negative)\n",
    "\n",
    "total_precision = (precision_positive + precision_neutral + precision_negative) / 3\n",
    "print('final precision score:', total_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c83635",
   "metadata": {},
   "source": [
    "Option:\n",
    "**Show all precision numbers. Explain that the neutral precision score brings it down by a lot. Find a resource of why this is.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d78020d",
   "metadata": {},
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6ac636",
   "metadata": {},
   "source": [
    "#### textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "626c26ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall-positive: 0.7945205479452054\n",
      "recall-neutral: 0.38095238095238093\n",
      "recall-negative: 0.3235294117647059\n",
      "final recall score: 0.49966744688743076\n"
     ]
    }
   ],
   "source": [
    "positive_tp = len(df[(df['sentiment_annotator'] == 'positive') & (df['sentiment_textblob'] == 'positive')])\n",
    "positive_fn = len(df[(df['sentiment_annotator'] == 'positive') & (df['sentiment_textblob'] != 'positive')])\n",
    "recall_positive = positive_tp / (positive_fn + positive_tp)\n",
    "\n",
    "\n",
    "neutral_tp = len(df[(df['sentiment_annotator'] == 'neutral') & (df['sentiment_textblob'] == 'neutral')])\n",
    "neutral_fn = len(df[(df['sentiment_annotator'] == 'neutral') & (df['sentiment_textblob'] != 'neutral')])\n",
    "recall_neutral = neutral_tp / (neutral_fn + neutral_tp)\n",
    "\n",
    "negative_tp = len(df[(df['sentiment_annotator'] == 'negative') & (df['sentiment_textblob'] == 'negative')])\n",
    "negative_fn = len(df[(df['sentiment_annotator'] == 'negative') & (df['sentiment_textblob'] != 'negative')])\n",
    "recall_negative = negative_tp / (negative_fn + negative_tp)\n",
    "\n",
    "\n",
    "\n",
    "print('recall-positive:', recall_positive)\n",
    "print('recall-neutral:', recall_neutral)\n",
    "print('recall-negative:', recall_negative)\n",
    "\n",
    "total_recall = (recall_positive + recall_neutral + recall_negative) / 3\n",
    "print('final recall score:', total_recall) # Find how to calculate this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0960fcae",
   "metadata": {},
   "source": [
    "#### vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6b981af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall-positive: 0.6438356164383562\n",
      "recall-neutral: 0.3333333333333333\n",
      "recall-negative: 0.5326797385620915\n",
      "final recall score: 0.5032828961112603\n"
     ]
    }
   ],
   "source": [
    "positive_tp = len(df[(df['sentiment_annotator'] == 'positive') & (df['sentiment_vader'] == 'positive')])\n",
    "positive_fn = len(df[(df['sentiment_annotator'] == 'positive') & (df['sentiment_vader'] != 'positive')])\n",
    "recall_positive = positive_tp / (positive_fn + positive_tp)\n",
    "\n",
    "\n",
    "neutral_tp = len(df[(df['sentiment_annotator'] == 'neutral') & (df['sentiment_vader'] == 'neutral')])\n",
    "neutral_fn = len(df[(df['sentiment_annotator'] == 'neutral') & (df['sentiment_vader'] != 'neutral')])\n",
    "recall_neutral = neutral_tp / (neutral_fn + neutral_tp)\n",
    "\n",
    "negative_tp = len(df[(df['sentiment_annotator'] == 'negative') & (df['sentiment_vader'] == 'negative')])\n",
    "negative_fn = len(df[(df['sentiment_annotator'] == 'negative') & (df['sentiment_vader'] != 'negative')])\n",
    "recall_negative = negative_tp / (negative_fn + negative_tp)\n",
    "\n",
    "\n",
    "\n",
    "print('recall-positive:', recall_positive)\n",
    "print('recall-neutral:', recall_neutral)\n",
    "print('recall-negative:', recall_negative)\n",
    "\n",
    "total_recall = (recall_positive + recall_neutral + recall_negative) / 3\n",
    "print('final recall score:', total_recall) # Find how to calculate this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c275a1",
   "metadata": {},
   "source": [
    "## f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51edefe",
   "metadata": {},
   "source": [
    "#### textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "754ba749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-positive: 0.3558282208588957\n",
      "f1-neutral: 0.3609022556390977\n",
      "f1-negative: 0.46588235294117647\n",
      "final f1 score: 0.3942042764797233\n"
     ]
    }
   ],
   "source": [
    "positive_tp = len(df[(df['sentiment_annotator'] == 'positive') & (df['sentiment_textblob'] == 'positive')])\n",
    "positive_fn = len(df[(df['sentiment_annotator'] == 'positive') & (df['sentiment_textblob'] != 'positive')])\n",
    "positive_fp = len(df[(df['sentiment_textblob'] == 'positive') & (df['sentiment_annotator'] != 'positive')])\n",
    "f1_positive = (2 * positive_tp) / (2 * positive_tp + positive_fp + positive_fn)\n",
    "\n",
    "neutral_tp = len(df[(df['sentiment_annotator'] == 'neutral') & (df['sentiment_textblob'] == 'neutral')])\n",
    "neutral_fn = len(df[(df['sentiment_annotator'] == 'neutral') & (df['sentiment_textblob'] != 'neutral')])\n",
    "neutral_fp = len(df[(df['sentiment_textblob'] == 'neutral') & (df['sentiment_annotator'] != 'neutral')])\n",
    "f1_neutral = (2 * neutral_tp) / (2 * neutral_tp + neutral_fp + neutral_fn)\n",
    "\n",
    "negative_tp = len(df[(df['sentiment_annotator'] == 'negative') & (df['sentiment_textblob'] == 'negative')])\n",
    "negative_fn = len(df[(df['sentiment_annotator'] == 'negative') & (df['sentiment_textblob'] != 'negative')])\n",
    "negative_fp = len(df[(df['sentiment_textblob'] == 'negative') & (df['sentiment_annotator'] != 'negative')])\n",
    "f1_negative = (2 * negative_tp) / (2 * negative_tp + negative_fp + negative_fn)\n",
    "\n",
    "\n",
    "print('f1-positive:', f1_positive)\n",
    "print('f1-neutral:', f1_neutral)\n",
    "print('f1-negative:', f1_negative)\n",
    "\n",
    "total_f1 = (f1_positive + f1_neutral + f1_negative) / 3\n",
    "print('final f1 score:', total_f1) # Find how to calculate this?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036c6a45",
   "metadata": {},
   "source": [
    "#### vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a112c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-positive: 0.3671875\n",
      "f1-neutral: 0.34146341463414637\n",
      "f1-negative: 0.6455445544554456\n",
      "final f1 score: 0.4513984896965306\n"
     ]
    }
   ],
   "source": [
    "positive_tp = len(df[(df['sentiment_annotator'] == 'positive') & (df['sentiment_vader'] == 'positive')])\n",
    "positive_fn = len(df[(df['sentiment_annotator'] == 'positive') & (df['sentiment_vader'] != 'positive')])\n",
    "positive_fp = len(df[(df['sentiment_vader'] == 'positive') & (df['sentiment_annotator'] != 'positive')])\n",
    "f1_positive = (2 * positive_tp) / (2 * positive_tp + positive_fp + positive_fn)\n",
    "\n",
    "neutral_tp = len(df[(df['sentiment_annotator'] == 'neutral') & (df['sentiment_vader'] == 'neutral')])\n",
    "neutral_fn = len(df[(df['sentiment_annotator'] == 'neutral') & (df['sentiment_vader'] != 'neutral')])\n",
    "neutral_fp = len(df[(df['sentiment_vader'] == 'neutral') & (df['sentiment_annotator'] != 'neutral')])\n",
    "f1_neutral = (2 * neutral_tp) / (2 * neutral_tp + neutral_fp + neutral_fn)\n",
    "\n",
    "negative_tp = len(df[(df['sentiment_annotator'] == 'negative') & (df['sentiment_vader'] == 'negative')])\n",
    "negative_fn = len(df[(df['sentiment_annotator'] == 'negative') & (df['sentiment_vader'] != 'negative')])\n",
    "negative_fp = len(df[(df['sentiment_vader'] == 'negative') & (df['sentiment_annotator'] != 'negative')])\n",
    "f1_negative = (2 * negative_tp) / (2 * negative_tp + negative_fp + negative_fn)\n",
    "\n",
    "\n",
    "print('f1-positive:', f1_positive)\n",
    "print('f1-neutral:', f1_neutral)\n",
    "print('f1-negative:', f1_negative)\n",
    "\n",
    "total_f1 = (f1_positive + f1_neutral + f1_negative) / 3\n",
    "print('final f1 score:', total_f1) # Find how to calculate this?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad590a66",
   "metadata": {},
   "source": [
    "## calculate using SKlearn metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5789790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the true sentiment labels and model predictions\n",
    "y_true = df['sentiment_annotator']\n",
    "y_textblob = df['sentiment_textblob']\n",
    "y_textblob2 = df['sentiment_textblob-v2']\n",
    "y_vader = df['sentiment_vader']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6b0619",
   "metadata": {},
   "source": [
    "#### textblob comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d27dc887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextBlob precision: 0.662684629948057\n",
      "TextBlob recall: 0.4095022624434389\n",
      "TextBlob f1 score: 0.4327427652216347\n"
     ]
    }
   ],
   "source": [
    "# calculate the precision, recall and f1 scores for TextBlob predictions\n",
    "precision_textblob, recall_textblob, f1_score_textblob, _ = precision_recall_fscore_support(y_true, y_textblob, average='weighted', labels=['negative', 'neutral', 'positive'])\n",
    "print(\"TextBlob precision:\", precision_textblob)\n",
    "print(\"TextBlob recall:\", recall_textblob)\n",
    "print(\"TextBlob f1 score:\", f1_score_textblob)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fad3e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextBlob 2 precision: 0.6592799216218422\n",
      "TextBlob 2 recall: 0.42081447963800905\n",
      "TextBlob 2 f1 score: 0.44922121601839377\n"
     ]
    }
   ],
   "source": [
    "# calculate the precision, recall and f1 scores for TextBlob predictions\n",
    "precision_textblob, recall_textblob, f1_score_textblob, _ = precision_recall_fscore_support(y_true, y_textblob2, average='weighted', labels=['negative', 'neutral', 'positive'])\n",
    "print(\"TextBlob 2 precision:\", precision_textblob)\n",
    "print(\"TextBlob 2 recall:\", recall_textblob)\n",
    "print(\"TextBlob 2 f1 score:\", f1_score_textblob)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92e72881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextBlob precision: [0.83193277 0.34285714 0.22924901]\n",
      "TextBlob recall: [0.32352941 0.38095238 0.79452055]\n",
      "TextBlob f1 score: [0.46588235 0.36090226 0.35582822]\n"
     ]
    }
   ],
   "source": [
    "# calculate the precision, recall and f1 scores for TextBlob predictions\n",
    "precision_textblob, recall_textblob, f1_score_textblob, _ = precision_recall_fscore_support(y_true, y_textblob, average=None, labels=['negative', 'neutral', 'positive'])\n",
    "print(\"TextBlob precision:\", precision_textblob)\n",
    "print(\"TextBlob recall:\", recall_textblob)\n",
    "print(\"TextBlob f1 score:\", f1_score_textblob)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dd98128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextBlob 2 precision: [0.828125   0.34177215 0.22553191]\n",
      "TextBlob 2 recall: [0.34640523 0.42857143 0.7260274 ]\n",
      "TextBlob 2 f1 score: [0.48847926 0.38028169 0.34415584]\n"
     ]
    }
   ],
   "source": [
    "# calculate the precision, recall and f1 scores for TextBlob predictions\n",
    "precision_textblob, recall_textblob, f1_score_textblob, _ = precision_recall_fscore_support(y_true, y_textblob2, average=None, labels=['negative', 'neutral', 'positive'])\n",
    "print(\"TextBlob 2 precision:\", precision_textblob)\n",
    "print(\"TextBlob 2 recall:\", recall_textblob)\n",
    "print(\"TextBlob 2 f1 score:\", f1_score_textblob)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4b6aad",
   "metadata": {},
   "source": [
    "#### Vader scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9013c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vader precision: [0.81909548 0.35       0.2568306 ]\n",
      "Vader recall: [0.53267974 0.33333333 0.64383562]\n",
      "Vader f1 score: [0.64554455 0.34146341 0.3671875 ]\n"
     ]
    }
   ],
   "source": [
    "# calculate the precision, recall and f1 scores for Vader predictions\n",
    "precision_vader, recall_vader, f1_score_vader, _ = precision_recall_fscore_support(y_true, y_vader, average=None, labels=['negative', 'neutral', 'positive'])\n",
    "print(\"Vader precision:\", precision_vader)\n",
    "print(\"Vader recall:\", recall_vader)\n",
    "print(\"Vader f1 score:\", f1_score_vader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2031e2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vader precision: 0.6593707012673834\n",
      "Vader recall: 0.5226244343891403\n",
      "Vader f1 score: 0.5562296748536596\n"
     ]
    }
   ],
   "source": [
    "# calculate the precision, recall and f1 scores for Vader predictions\n",
    "precision_vader, recall_vader, f1_score_vader, _ = precision_recall_fscore_support(y_true, y_vader, average='weighted', labels=['negative', 'neutral', 'positive'])\n",
    "print(\"Vader precision:\", precision_vader)\n",
    "print(\"Vader recall:\", recall_vader)\n",
    "print(\"Vader f1 score:\", f1_score_vader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be05793e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
